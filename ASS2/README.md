#### q_learning中使用了epsilon-greedy, 在最初只使用greedy的时候导致Qtable中有些值没有得到有效更新，导致模型一直重复动作，陷入循环
#### 之后改用epsilon-greedy并且增加了episodes数量，使得Qtable得到充分更新，在检验时不会再出现循环情况
#### Sarsa中使用了softmax，使得Qtable中值都可以得到有效更新，不会出现验证时陷入循环的情况
#### 多错多改，多错是好事。
#### α意味着新信息替代旧信息的成都，α越大，替代越快
#### γ意味着未来值对当前Q值的影响，较高的折扣因子表示未来的奖励更重要